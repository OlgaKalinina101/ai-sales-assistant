# AI Sales Assistant

## Обзор проекта

AI Sales Assistant — это приложение для автоматической генерации персонализированных деловых писем на основе данных о клиентах и базе знаний. Проект использует семантический поиск (RAG) и большие языковые модели (LLM) для создания убедительных писем, адаптированных под сегмент компании. Основные компоненты включают обработку документов, векторную базу данных ChromaDB, LangGraph для управления конвейером и FastAPI для предоставления API.

**Бизнес-цель**: Повысить вероятность отклика клиентов за счет персонализированных писем, основанных на релевантных кейсах из базы знаний.

## Структура директорий

```
AI_sales_assistant/
│
├── app/                           # Основная логика приложения
│   ├── __init__.py
│   ├── routes.py                  # FastAPI маршруты
│   │
│   ├── letter_pipeline/          # Логика LangGraph пайплайна
│   │   ├── __init__.py
│   │   ├── graph.py              # Сборка графа LangGraph
│   │   ├── nodes.py              # Отдельные шаги пайплайна
│   │   ├── openai_client.py      # Настройка клиента OpenAI
│   │   ├── prompt_template.txt   # Контекстный промпт для модели
│   │   └── types.py              # Типы состояния пайплайна
│   │
│   ├── retrieval.py              # Извлечение данных из ChromaDB
│   └── helpers.py                # Хелпер для очистки json
│
├── data/                         # Данные проекта
│   ├── raw/                      # Необработанные файлы
│   └── processed/                # Распакованные файлы
│
├── data_ingestion/              # Обработка документов
│   ├── __init__.py
│   ├── config.py               # Пути к директориям/моделям
│   ├── cleaner.py              # Очистка директорий
│   ├── extractor.py            # Извлечение данных 
│   ├── ingestor.py             # Объединение в пайплайн
│   └── loader.py               # Загрузка в память
│
├── utils/                       # Утилиты общего назначения
│   ├── __init__.py
│   ├── chroma_client.py        # Инициализация подключения к Chroma
│   └── logger.py               # Логгер
│
├── vector_store/               # Векторная база данных
│   └── chroma.sqlite3          # SQLite-файл ChromaDB
│
├── .env                        # Переменные окружения
├── Dockerfile                  # Dockerfile для сборки 
├── docker-compose.yml          # Docker Compose для запуска
├── requirements.txt            # Python-зависимости
├── main.py                     # Точка входа для FastAPI
└── README.md                   # Описание проекта

```

## Пайплайн обработки данных

1. **Распаковка данных**:
   - Вызывается `extract_nested_zip("data/raw/Konsol_Pro_Articles.zip", "data/processed")` для распаковки архива с учетом трех уровней вложенности.
   - Файлы переименованы в `snake_case` (например, `Konsol_Pro_Articles.zip`) для предотвращения ошибок в путях.

2. **Создание базы знаний**:
```python
   from data_ingestion.ingestor import KnowledgeBaseBuilder
   builder = KnowledgeBaseBuilder()
   builder.ingest()
```
   - Загружает `.md` файлы из `data/processed` и PDF (`data/raw/Service_Console.pdf`).
   - Разбивает документы на чанки (256–384 токена, перекрытие 40–64 токена).
   - Создает эмбеддинги с использованием `sentence-transformers/all-MiniLM-L6-v2`.
   - Сохраняет чанки и эмбеддинги в ChromaDB.

3. **Генерация письма**:
   - FastAPI эндпоинт `/generate_email` принимает пользовательский ввод.
   - LangGraph конвейер выполняет поиск релевантных чанков, формирует промпт и генерирует письмо через `gpt-4o`.

## Обработка источников

### Markdown файлы
- Обнаружено 15 `.md` файлов в архиве `Konsol_Pro_Articles.zip`.
- Обработка с помощью llama_index MarkdownReader. Оптимальный размер чанка: 256–384 токена, перекрытие 40–64 токена. 
- **Почему**:
  - Короткие блоки текста (2–4 абзаца) сохраняют смысловую целостность (клиент, проблема, решение).
  - Снижение стоимости эмбеддингов.
  - Точный RAG с подходом "one-fact-per-chunk".

### PDF документ
- Обработка `Service_Console.pdf` с помощью `pdfplumber`.
- **Проблема**: Предупреждения о некорректном `FontBBox` в шрифтах. Визуальная проверка чанков показала, что текст извлечен корректно, предупреждения некритичны.
- **Рекомендация**: Для большого числа PDF с подобными шрифтами добавить обработку исключений или предобработку шрифтов.

### Модель эмбеддингов
- Использована `sentence-transformers/all-MiniLM-L6-v2`.
- **Преимущества**:
  - Легкая (~80 МБ), быстрая, работает на CPU.
  - Подходит для деловых текстов и семантического поиска.
- **Альтернативы**:
  | Модель                        | Преимущества                              | Недостатки                              |
  |------------------------------|-------------------------------------------|-----------------------------------------|
  | multi-qa-mpnet-base-dot-v1   | Точнее для Q&A, лучше на длинных текстах  | Медленнее, больше памяти                |
  | BAAI/bge-small-en            | Хороший баланс качества и скорости         | Требует ручного нормирования            |
  | intfloat/e5-small-v2         | Новая, для open-domain retrieval          | Требует формата query/passage           |
  | text-embedding-3-small       | Высокое качество                          | Платная, требует API                    |

## Компоненты проекта
Логика загрузки и сохранения данных в БД реализована как отдельный модуль data_ingestion, в дальнейшем может быть изолирована от основного приложения.

### KnowledgeBaseBuilder
Класс для создания базы знаний:
- Загружает `.md` и PDF файлы.
- Разбивает на чанки (256–384 токена, перекрытие 40–64).
- Создает эмбеддинги (`sentence-transformers/all-MiniLM-L6-v2`).
- Сохраняет в ChromaDB.
- **Оптимизации памяти**:
  - Итеративная обработка документов через `yield`.
  - Пакетная обработка чанков (батчи по 100).
  - Очистка памяти (`del` для временных списков).
  - Мониторинг с `psutil` после создания эмбеддингов и в конце.


**Пример вызова**:
```python
from data_ingestion.ingestor import KnowledgeBaseBuilder
builder = KnowledgeBaseBuilder()
builder.ingest()
```

### Очистка директорий
Функция `clear_directory` и обертки `clear_raw_data`, `clear_processed_data`:
- Используют `os.scandir` для итеративной обработки, минимизируя память.
- Логируют удаление файлов и папок.
- Обрабатывают ошибки через try-except.

**Пример вызова**:
```python
from data_ingestion.cleaner import clear_processed_data
clear_processed_data()
```

### Семантический поиск (RAG)
Функция `find_relevant_chunks_by_segment`:
- Выполняет поиск чанков в ChromaDB по сегменту.
- **Оптимизации памяти**:
  - Валидация `segment` и `top_k`.
  - Использует глобальный экземпляр `SentenceTransformer`.
- **Рекомендации**:
  - Добавить синонимы или fuzzy-поиск для сегментов.
  - Хранить нормализованные сегменты в метаданных.

**Пример вызова**:
```python
from app.retrieval import find_relevant_chunks_by_segment
chunks = find_relevant_chunks_by_segment("маркетинговое агентство", collection, embedder)
```

### LangGraph пайплайн

Пайплайн построен с использованием LangGraph: граф включает ноды input, search, prompt, generate, output, каждая из которых изолирует ответственность по принципу SRP.

Конвейер для генерации письма состоит из узлов:
| Узел            | Назначение                              |
|-----------------|-----------------------------------------|
| `input`         | Принимает `user_input`                  |
| `search`        | Ищет релевантные чанки по сегменту      |
| `prompt`        | Формирует промпт для LLM                |
| `generate`      | Генерирует письмо через `gpt-4o`        |
| `output`        | Возвращает итоговое письмо              |

- **Оптимизации памяти**:
  - Глобальные ресурсы (`SentenceTransformer`, ChromaDB, `AsyncOpenAI`).
  - Валидация данных на каждом узле.
  - Мониторинг с `psutil` в `search` и `generate`.

### FastAPI эндпоинт
Эндпоинт `/generate_email` принимает JSON:
```json
{
  "user_input": {
    "контакт": "Мозякин Никита",
    "должность": "Технический директор",
    "название_компании": "FIVE",
    "сегмент": "маркетинговое агентство"
  }
}
```

**Пример ответа**:
```json
{
  "subject": "Повышение эффективности работы с маркетинговыми партнерами через Консоль",
  "letter": "Уважаемый Никита,\n\nМеня зовут [Ваше Имя], я представляю платформу Консоль — инновационное решение для автоматизации документооборота и управления взаимоотношениями с партнерами в сфере маркетинга.\n\nМы понимаем, что в маркетинговом агентстве эффе
ктивная коммуникация и оперативный документооборот с подрядчиками и блогерами играют ключевую роль. На примере наших клиентов, таких как MEDIAR, которые успешно работают через Консоль с более чем 250 эксклюзивными блогерами, мы видим, как автоматизация может з
начительно упростить и ускорить рабочие процессы.\n\nКроме того, компания WONDER LAB с помощью нашей платформы смогла привлечь более тысячи блогеров, оптимизировав свои процессы и сконцентрировавшись на важнейших аспектах бизнеса.\n\nМы уверены, что Консоль мо
жет стать для FIVE надежным инструментом, позволяющим повысить эффективность работы и сократить время на бюрократические процедуры.\n\nПредлагаем вам протестировать нашу платформу и оценить все ее преимущества самостоятельно. Мы будем рады организовать встречу или демонстрацию возможностей Консоли в любое удобное для вас время.\n\nС уважением,\n[Ваше Имя]\n[Ваша Должность]\nКонсоль"
}
```

- **Оптимизации памяти**:
  - Pydantic с `max_length` для валидации.
  - Обрезка логов до 500–1000 символов.
  - Мониторинг с `psutil`.
- **Обработка ошибок**: 
  - Возврат `HTTPException` при сбоях.
  - Обработка невалидного json в случаях, когда модель возвращает json с оберткой. 

## 🧠 Промпт-инжиниринг
Для быстрой демонстрации результата выбрана модель `gpt-4o` с настройками: 
```
        response = await openai_client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {
                    "role": "system",
                    "content": "Ты — AI-помощник, генерирующий персонализированные письма для клиентов.",
                },
                {"role": "user", "content": state["prompt"]},
            ],
            temperature=0.7,
        )
```
temperature=0.7 выбрана для генерации креативного, персонализированного письма.

Контекстуализированный ввод:
Используем структурированные поля (контакт, должность, название_компании, сегмент) для имитации CRM-данных, что повышает персонализацию и снижает шум в генерации.

Инжекция знаний:
Интегрируем заранее извлечённый контекст из базы знаний (векторный поиск) для увеличения релевантности, чтобы модель могла ссылаться на реальные кейсы.

Пошаговая инструкция в prompt'e:
Вместо абстрактного запроса применена пошаговая разметка:

- представление от имени компании,
- выгода для получателя,
- ссылку на похожие кейсы,
- призыв к действию — всё через bullet-пункты.
- 
Это снижает вероятность галлюцинаций и улучшает логическую структуру письма.

Фокус на деловом B2B-тоне:
Задано ограничение: письмо должно быть «персонализированным и убедительным, но не навязчивым», чтобы не вызывать отторжение у холодного клиента.

Явная JSON-структура в ответе:
Модель направляется к возвращению строго формализованного JSON, содержащего subject и body. Это важно для автоматической обработки и минимизации пост-обработки.

Почему работает:
✅ Снижение ошибок генерации за счёт чёткой структуры запроса и указания формата вывода.
✅ Сильная персонализация через явные поля — работает как имитация CRM-поля.
✅ Реалистичность и обоснованность письма через встраивание фактического контекста.
✅ Повышение конверсии за счёт релевантной темы письма (subject), сфокусированной на должности и интересах адресата.
✅ Гибкость: можно использовать шаблон в разных бизнес-доменах, меняя только поля и контекст.

## Запуск проекта

### Локальный запуск
1. Установите зависимости:
   ```bash
   pip install -r requirements.txt
   ```
2. Настройте `.env`:
   ```plaintext
   OPENAI_API_KEY=sk-...
   ```
3. Запустите приложение:
   ```bash
   uvicorn main:app --reload
   ```

### Запуск через Docker
1. Собрать и запустить сервисы:
   ```bash
   docker-compose up --build
   ```
2. Доступ к API: `http://localhost:8000/generate_email`.

## Дополнительные рекомендации
- **Улучшение RAG**:
  - Добавить синонимы или fuzzy-поиск для сегментов.
  - Хранить нормализованные сегменты в метаданных.
- **Обработка PDF**:
  - Добавить предобработку шрифтов для устранения предупреждений `FontBBox`.
- **Кэширование**:
  - Кэшировать результаты поиска чанков для повторяющихся сегментов:
    ```python
    from functools import lru_cache
    @lru_cache(maxsize=100)
    def cached_search(segment: str) -> tuple:
        return tuple(find_relevant_chunks_by_segment(segment, collection, embedder))
    ```
- **Тестирование**:
  - Добавить интеграционные тесты с `TestClient` для `/generate_email`.
- **Guardialis**:
  - Добавить валидацию полученного ответа от модели с помощью Guardrails для проверки фактической точности, стилистической нейтральности или ограничения длины ответа.
- **Безопасность**:
  - Добавить аутентификацию API (например, JWT или API-ключ).
